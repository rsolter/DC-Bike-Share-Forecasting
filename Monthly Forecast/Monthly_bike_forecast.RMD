---
output: 
  md_document:
    variant: markdown_github
---

# Forecasting Capital Bikeshare usage with ARIMA


***** 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,warning = F,message = F,cache = T, error=F, fig.width = 9)

# Libraries
  library(tidyverse)
  library(lubridate)
  library(scales)
  library(tidyquant)
  library(quantmod)
  library(forecast)
  library(tseries)
  library(TSstudio)
  library(ggpubr)

# Reading in NOAA.csv with average temperature data for DC
noaa <- read.csv(file="~/Personal Git/DC-Bike-Share-Forecasting/Monthly Forecast/USW00013743.csv")
noaa <- noaa %>% select(STATION,DATE,LATITUDE,LONGITUDE,ELEVATION,NAME,TAVG,TAVG_ATTRIBUTES)

noaa$TAVG <- (((noaa$TAVG/5)*9)+32) 

noaa$Month <- substr(noaa$DATE,6,7) %>% as.numeric()
noaa$Year <- substr(noaa$DATE,1,4) %>% as.numeric()

noaa <- noaa %>% select(Year,Month,TAVG) 

noaa <- noaa %>% 
  filter(Year>=2010) %>% 
  filter(Year<2019)

noaa <- noaa[10:104, ]  

noaa_ts <- ts(noaa$TAVG,
                  start=c(2010,10), end=c(2018,8), 
                  frequency = 12)
  

# Loading cleaned bike data from Oct, 2010 - Aug, 2018 
  # source: (https://s3.amazonaws.com/capitalbikeshare-data/index.html)

load(file="bike_trips.rdata")
monthly_bike_trips <- bike_trips %>% filter(Date>="2010-10-01") %>% filter(Date<"2018-09-01")
  
monthly_bike_trips <- monthly_bike_trips %>%
    mutate(Year=lubridate::year(Date),
           Month=lubridate::month(Date)) %>%
    group_by(Year,Month) %>%
    summarise(Monthly_Trips=sum(n,na.rm=T))

ts_month <- ts(monthly_bike_trips$Monthly_Trips,
                  start=c(2010,10), end=c(2018,8), 
                  frequency = 12)
  

# links 
  # https://towardsdatascience.com/time-series-analysis-with-auto-arima-in-r-2b220b20e8ba
  # https://people.duke.edu/~rnau/411arim.htm
  # https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322

```



Having used DC Capital BikeShare data [previously](https://rsolter.github.io/r/forecasting/Monthly_Bike_Forecast_ETS/) to forecast with exponential smoothing models, I wanted to do the same with ARIMA modeling. ARIMA a complementary method for forecasting univariate timeseries, but it also allows for explanatory variables in the regression. In this case, Iâ€™ve added monthly average temperatures as a regressor. The [data](https://www.ncei.noaa.gov/access/search/data-search/global-summary-of-the-month) was downloaded from NOAA and represents measurements from the Regan National Airport weather station.


****

# Tools for Stationarity

A requirement of ARIMA modeling is stationarity of the series which is achieved by having _time invariant_ mean, variance, and co-variance of the series. Any time series with a trend or seasonality is not stationary. 
  
One tool used to achieve stationarity is **differencing.** or computing the difference between consecutive observations. As an example, we can show this using stock data downloaded using the **quantmod** R package. By differencing the data by just one observation, the trend in the stock prices completely disappears:

```{r differencing example, echo=FALSE}
options("getSymbols.warning4.0"=FALSE)
options("getSymbols.yahoo.warning"=FALSE)

quantmod::getSymbols("AAPL", 
                     from = '2017-01-01',
                     to ='2017-06-01', 
                     warnings=FALSE,auto.assign=TRUE)

closingApple <- AAPL$AAPL.Close
DclosingApple <- diff(closingApple,1)
DclosingApple <- DclosingApple[2:length(DclosingApple)] # removing NA


#layout(matrix(c(1,2),2,1),byrow=TRUE)
graphics::par(mfrow=c(2,1))
plot(closingApple, main ="Closing Prices on Apple Stock")
plot(DclosingApple, main ="Differenced Closing Prices on Apple Stock")

```


However, was that enough to achieve stationarity? Formally, stationarity can be assessed by using one of many unit tests, one of which is the Kwiatkowski-Phillips-Schmidt-Shin (**KPSS**) test. The null hypothesis for the KPSS test is that the data are stationary, and we look for evidence that the null hypothesis is false. Consequently, small p-values (e.g., less than 0.05), indicate the data is not stationary and suggest differencing is required. Read more [here](https://nwfsc-timeseries.github.io/atsa-labs/sec-boxjenkins-aug-dickey-fuller.html).

In the first test on the raw data, the null hypothesis of stationarity is rejected. Run a second time on differenced data, the null hypothesis is not rejected, indicating that our differencing worked!

```{r adf test}

tseries::kpss.test(closingApple) # p-value 0f 0.01

tseries::kpss.test(DclosingApple) # p-value 0f 0.1

#urca::ur.kpss(closingApple) %>% summary()  
#urca::ur.kpss(DclosingApple) %>% summary() # p-value > 0.1 - Stationary!

```



Standard variance is another requirement for stationarity. Handling non-standard variance is done by transforming the time series, either with logarithmic functions or the Box-Cox transformation. To show it in action, I'll create a fake dataset whose variance grows over time:

```{r heteroskedasticity}
trend <- ts(seq(from = 10, to = 110))
cycle <- ts(sin(trend)) * 0.2 * trend
tseries_h <- trend + cycle
plot.ts(tseries_h)
```

And use BoxCox.lambda() to determine the correct lambda for transforming the data to get a constant variance:

```{r Box-Cox}
lambda <- BoxCox.lambda(tseries_h)
plot.ts(BoxCox(tseries_h, lambda = lambda))
```


**ARIMA Theory**

ARIMA is an acronym for Auto Regressive (AR) Integrated (I) Moving Average (MA) and can be implemented in R using the automated function `auto.arima` from the forecast package. ARIMA modeling is best understood by breaking it into its components:

**Auto-regressive** models forecast using a linear combination of past values of the variable. An AR model of order _p_ can be written as:

$$y_{t} = c + \theta y_{t-1} + \theta y_{t-2} + ... + \theta y_{t-p} + \epsilon_{t}$$

Where $\epsilon$ is white noise and the model is denoted **AR(_p_)**, an auto-regressive model of order p. Read more [here](https://otexts.com/fpp2/AR.html)


**Moving Average** models use past values forecast errors as regressors for forecasting:

$$y_{t} = c + \epsilon_{t} + \theta_{1} \epsilon_{t-1} + \theta_{2} \epsilon_{t-2} + ... + \theta_{q} \epsilon_{t-q}$$
Again, in this model, $\epsilon$ is white noise and the model is denoted **MA(_q_)**, a moving-average model of order q. Read more [here](https://otexts.com/fpp2/MA.html) 



**ARIMA Formulation** 

Combining the autoregression and moving average model, we obtain a non-seasonal ARIMA model which is denoted as **ARIMA**(_p,d,q_)**model**, where 
  
  * p - order of the auto-regressive part
  * d - degree of first differencing involved
  * q - order of the moving average part
  
The **seasonal** version of ARIMA builds upon this to include 4 additional seasonal terms:

  * m - number of observations per year (12 for months, 4 for quarters, etc.)
  * P - order of the seasonal auto-regressive part
  * D - degree of first differencing involved for seasonal observations
  * Q - order of the seasonal moving average part



****

#### Application to DC Bike Share


Now we'll apply the ARIMA model to the monthly bike share data while using the temperature data as a regressor. 

From the initial plot of the bike data, we can tell the data has clear seasonality with many fewer riders in the winter months. At the same time, we see an growth in the number of total riders that has appeared to slow in recent years. In the plot for temperature we see a predicatble seasonality whose peaks and valleys remain almost equal in size throughout the entire time frame: 

```{r partition, echo=FALSE}
#ts_month
ts_month_partition <- TSstudio::ts_split(ts_month,sample.out = 12)
bike_train <- ts_month_partition$train
bike_test <- ts_month_partition$test

#noaa_ts
noaa_ts_partition <- TSstudio::ts_split(noaa_ts,sample.out = 12)
noaa_train <- noaa_ts_partition$train
noaa_test <- noaa_ts_partition$test

```


```{r, Train Plots}
p1 <- autoplot(bike_train) +
  xlab("Year") + ylab("Total Rides") + scale_y_continuous(label=comma) +
  #ggtitle("Monthly bike rentals") + 
  theme_minimal() + xlab("") +
  theme(plot.title = element_text(hjust = 0.5))


p2 <- autoplot(noaa_train) +
  xlab("Year") + ylab("Monthly Temperature") +
  #ggtitle("Monthly Temperature (F)") + 
  theme_minimal() + xlab("") +
  theme(plot.title = element_text(hjust = 0.5)) + 
  labs(caption = "Validation data not included")


figure <- ggarrange(p1, p2,
                    labels = c("Bike Rides", "Temperature"),
                    ncol = 1, nrow = 2)
figure

```



The function **auto.arima()** automates the process of choosing the parameters. ['The auto.arima() function in R uses a variation of the Hyndman-Khandakar algorithm, which combines unit root tests, minimisation of the AICc and MLE to obtain an ARIMA model'](https://otexts.com/fpp2/arima-r.html). 

A seasonal ARIMA model produces a MAPE of 9.81% on the training set and a similar MAPE on the test set (9.6%)
```{r arima1}
arima_1 <- auto.arima(bike_train,seasonal = TRUE)
summary(arima_1)
fr <- forecast(arima_1,h = 12)

#mean((fr$mean-bike_test)/fr$mean)


autoplot(fr) +
  autolayer(bike_test, series="Actual", PI=FALSE) +
  scale_y_continuous(label=comma) + theme_minimal() +
  xlab(" ") +
  ylab("") +
  ggtitle("Forecasts from ARIMA(0,1,4)(0,1,0)[12]") +
  guides(colour=guide_legend(title="Forecast")) +
  theme(legend.position = "bottom")


# tsdisplay(residuals(arima_1), lag.max=15, main='auto.arima() Model Residuals')

```



Second attempt at auto.arima() with NOAA data used as a regressor, results in slightly worse performance. The training MAPE is 11.9% and the test MAPE is 10.6%. Why might this be? It could be that the temperature isn't actually a good predictor of ridership, at least at a monthly level. Referring to the charts above, it's clear that the temperature data reflects the seasonality in the data, but that's also built into our auto.arima() function with the seasonal="TRUE" argument and m=12 parameter, so perhaps avg. monthly temperature is unecessary for building our model.

```{r arima2}
arima_2 <- auto.arima(bike_train,seasonal = TRUE,xreg = noaa_train)
summary(arima_2)
fr2 <- forecast(arima_2,h = 12,xreg = noaa_test)

#mean((fr2$mean-bike_test)/fr2$mean)
#tsdisplay(residuals(arima_2), lag.max=15, main='auto.arima() + NOAA Regressors Model Residuals')

autoplot(fr2) +
  autolayer(bike_test, series="Actual", PI=FALSE) +
  scale_y_continuous(label=comma) + theme_minimal() +
  xlab(" ") +
  ylab("") +
  ggtitle("Forecasts from ARIMA(0,1,2)(1,0,0)[12] and NOAA temperature regressor") +
  guides(colour=guide_legend(title="Forecast")) +
  theme(legend.position = "bottom")

```



